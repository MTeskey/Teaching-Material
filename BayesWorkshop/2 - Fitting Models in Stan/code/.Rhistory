nrow = length(a_grid),
ncol = length(b_grid))
for (i in 1:length(a_grid)){
for (j in 1:length(b_grid)){
ll_grid[i,j] = reg_loglik(x, y, a_grid[i], b_grid[j])
}
}
?max
a_grid = seq(from = a - 1, to = a + 1, length.out = 100)
b_grid = seq(from = b - 1, to = b + 1, length.out = 100)
ll_grid = matrix(NA,
nrow = length(a_grid),
ncol = length(b_grid))
for (i in 1:length(a_grid)){
for (j in 1:length(b_grid)){
ll_grid[i,j] = reg_loglik(x, y, a_grid[i], b_grid[j])
}
}
max(ll_grid)
ll_grid = ll_grid / max(ll_grid)
max(ll_grid)
ll_grid / max(ll_grid)
y - a - b*x
ll = sum( -(y - a - b*x)^2 )
ll
# Computes and plots the log likelihood function for a simple
# linear regression model. For simplicity, we assume that the
# error variance is known to be equal to sqrt(N)
# Sample size
N = 100
# Intercept
a = 0
# Slope
b = 1
# Error variance
sigma = sqrt(N)
# Simulate data
x = 1:N
y = rnorm(N, mean = a + b*x, sd = sigma)
# Plot regression data
par(cex = 1.2)
plot(x, y,
xlab = 'X', ylab = 'Y',
main = 'Simulated Regression Data',
pch = 16)
# Log likelihood function for parameters a and b
reg_loglik = function(x, y, a, b){
ll = sum( -(y - a - b*x)^2 / 10000)
return(ll)
}
# Prepare likelihood surface
a_grid = seq(from = a - 1, to = a + 1, length.out = 100)
b_grid = seq(from = b - 1, to = b + 1, length.out = 100)
ll_grid = matrix(NA,
nrow = length(a_grid),
ncol = length(b_grid))
for (i in 1:length(a_grid)){
for (j in 1:length(b_grid)){
ll_grid[i,j] = reg_loglik(x, y, a_grid[i], b_grid[j])
}
}
ll_grid = ll_grid / max(ll_grid)
# Plot likelihood surface
image(a_grid, b_grid, ll_grid)
min(ll_grid)
max(ll_grid)
# Prepare likelihood surface
a_grid = seq(from = a - 1, to = a + 1, length.out = 100)
b_grid = seq(from = b - 1, to = b + 1, length.out = 100)
ll_grid = matrix(NA,
nrow = length(a_grid),
ncol = length(b_grid))
for (i in 1:length(a_grid)){
for (j in 1:length(b_grid)){
ll_grid[i,j] = reg_loglik(x, y, a_grid[i], b_grid[j])
}
}
ll_grid = ll_grid / min(ll_grid)
# Plot likelihood surface
image(a_grid, b_grid, ll_grid)
min(ll_grid)
max(ll_grid)
# Log likelihood function for parameters a and b
reg_loglik = function(x, y, a, b){
ll = sum( -(y - (a + b*x))^2)
return(ll)
}
# Prepare likelihood surface
a_grid = seq(from = a - 1, to = a + 1, length.out = 100)
b_grid = seq(from = b - 1, to = b + 1, length.out = 100)
ll_grid = matrix(NA,
nrow = length(a_grid),
ncol = length(b_grid))
for (i in 1:length(a_grid)){
for (j in 1:length(b_grid)){
ll_grid[i,j] = reg_loglik(x, y, a_grid[i], b_grid[j])
}
}
ll_grid = ll_grid / min(ll_grid)
# Plot likelihood surface
image(a_grid, b_grid, ll_grid)
plot(1:100, ll_grid[,50], type = 'l')
plot(1:100, ll_grid[50,], type = 'l')
ll_grid
plot(1:100, ll_grid[,50], type = 'l')
a_grid = seq(from = a - 1, to = a + 1, length.out = 100)
b_grid = seq(from = b - 1, to = b + 1, length.out = 100)
ll_grid = matrix(NA,
nrow = length(a_grid),
ncol = length(b_grid))
for (i in 1:length(a_grid)){
for (j in 1:length(b_grid)){
ll_grid[i,j] = reg_loglik(x, y, a_grid[i], b_grid[j])
}
}
ll_grid = -ll_grid / max(-ll_grid)
# Plot likelihood surface
image(a_grid, b_grid, ll_grid)
# Number of coin flips
N = 4
# Number of heads
k = 3
# Likelihood function
loglik = function(p) k*log(p) + (N-k)*log(1-p)
# Plot
x = seq(from = 0, to = 1, length.out = 100)
plot(x, loglik(x), xlim = c(0,1),
type = 'l', lwd = 2,
xlab = 'p', ylab = 'Log-Likelihood',
main = 'Binomial Log-Likelihood')
abline(h = k/N, lty = 2)
abline(v = k/N, lty = 2)
abline(v = k/N, lty = 2, lwd = 2, col = 'red')
grid(nx = NULL, ny = NULL)
lik = function(p) choose(N,k) * p^k * (1-p)^(N-k)
# Plot
x = seq(from = 0, to = 1, length.out = 100)
plot(x, lik(x), xlim = c(0,1),
type = 'l', lwd = 2,
xlab = 'p', ylab = 'Log-Likelihood',
main = 'Binomial Log-Likelihood')
grid(nx = NULL, ny = NULL)
# Indicate maximum likelihood estimate
abline(v = k/N, lty = 2, lwd = 2, col = 'red')
### LIKELIHOOD ###
lik = function(p) choose(N,k) * p^k * (1-p)^(N-k)
# Plot
x = seq(from = 0, to = 1, length.out = 100)
plot(x, lik(x), xlim = c(0,1),
type = 'l', lwd = 2,
xlab = 'p', ylab = 'Likelihood',
main = 'Binomial Likelihood')
grid(nx = NULL, ny = NULL)
# Indicate maximum likelihood estimate
abline(v = k/N, lty = 2, lwd = 2, col = 'red')
### LOG LIKELIHOOD ###
loglik = function(p) k*log(p) + (N-k)*log(1-p)
# Plot
x = seq(from = 0, to = 1, length.out = 100)
plot(x, loglik(x), xlim = c(0,1),
type = 'l', lwd = 2,
xlab = 'p', ylab = 'Log-Likelihood',
main = 'Binomial Log-Likelihood')
grid(nx = NULL, ny = NULL)
# Indicate maximum likelihood estimate
abline(v = k/N, lty = 2, lwd = 2, col = 'red')
3!
?factorial
# Computes and plots the log/likelihood function for a binomial experiment
# Number of coin flips
N = 4
# Number of heads
k = 3
# Normalized likelihood
lik = function(p){
p^k * (1-p)^(N-k) * (factorial(N+1)/(factorial(k)*factorial(n-k)))
}
# Beta prior parameters
alpha = 1
beta = 1
# Plot
x = seq(from = 0, to = 1, length.out = 100)
plot(x, lik(x), xlim = c(0,1),
type = 'l', lwd = 2,
xlab = 'p', ylab = '',
main = 'Bayesian Binomial Model')
grid(nx = NULL, ny = NULL)
lines(x, dbeta(x, alpha, beta),
lty = 2, lwd = 2, col = 'blue')
lik = function(p){
p^k * (1-p)^(N-k) * (factorial(N+1)/(factorial(k)*factorial(N-k)))
}
# Beta prior parameters
alpha = 1
beta = 1
# Plot
x = seq(from = 0, to = 1, length.out = 100)
plot(x, lik(x), xlim = c(0,1),
type = 'l', lwd = 2,
xlab = 'p', ylab = '',
main = 'Bayesian Binomial Model')
grid(nx = NULL, ny = NULL)
lines(x, dbeta(x, alpha, beta),
lty = 2, lwd = 2, col = 'blue')
lines(x, dbeta(x, alpha + k, beta + N - k),
lty = 2, lwd = 2, col = 'red')
# Number of coin flips
N = 4
# Number of heads
k = 3
# Normalized likelihood
lik = function(p){
p^k * (1-p)^(N-k) * (factorial(N+1)/(factorial(k)*factorial(N-k)))
}
# Beta prior parameters
alpha = 5
beta = 5
# Plot likelihood
x = seq(from = 0, to = 1, length.out = 100)
plot(x, lik(x), xlim = c(0,1),
type = 'l', lwd = 2,
xlab = 'p', ylab = '',
main = 'Bayesian Binomial Model')
grid(nx = NULL, ny = NULL)
# Plot prior
lines(x, dbeta(x, alpha, beta),
lty = 2, lwd = 2, col = 'blue')
# Plot posterior
lines(x, dbeta(x, alpha + k, beta + N - k),
lty = 2, lwd = 2, col = 'red')
plot(NULL, xlim = c(0,1),
type = 'l', lwd = 2,
xlab = 'p', ylab = '',
main = 'Bayesian Binomial Model')
grid(nx = NULL, ny = NULL)
plot(0, xlim = c(0,1),
type = 'l', lwd = 2,
xlab = 'p', ylab = '',
main = 'Bayesian Binomial Model')
grid(nx = NULL, ny = NULL)
# Plot likelihood
x = seq(from = 0, to = 1, length.out = 100)
plot(0,
xlim = c(0,1), ylim = c(0, max(dbeta(x, alpha + k, beta + N - k)))
type = 'l', lwd = 2,
xlab = 'p', ylab = '',
main = 'Bayesian Binomial Model')
grid(nx = NULL, ny = NULL)
x = seq(from = 0, to = 1, length.out = 100)
plot(0,
xlim = c(0,1), ylim = c(0, max(dbeta(x, alpha + k, beta + N - k))),
type = 'l', lwd = 2,
xlab = 'p', ylab = '',
main = 'Bayesian Binomial Model')
grid(nx = NULL, ny = NULL)
lines(x, dbeta(x, alpha, beta),
lty = 2, lwd = 2, col = 'blue')
lines(x, lik(x),
lty = 2, lwd = 2, col = 'blue')
lines(x, lik(x),
lty = 1, lwd = 2, col = 'black')
lines(x, dbeta(x, alpha + k, beta + N - k),
lty = 2, lwd = 2, col = 'red')
# Number of coin flips
N = 4
# Number of heads
k = 3
# Normalized likelihood
lik = function(p){
p^k * (1-p)^(N-k) * (factorial(N+1)/(factorial(k)*factorial(N-k)))
}
Rcpp::sourceCpp('Desktop/R_entropy/similarity_count.cpp')
Rcpp::sourceCpp('Desktop/R_entropy/similarity_count.cpp')
Rcpp::sourceCpp('Desktop/R_entropy/similarity_count.cpp')
Rcpp::sourceCpp('Downloads/Matlab_Multivariate_Multiscale_Entropy/rcpp_test.cpp')
Rcpp::sourceCpp('Downloads/Matlab_Multivariate_Multiscale_Entropy/rcpp_test.cpp')
Rcpp::sourceCpp('Downloads/Matlab_Multivariate_Multiscale_Entropy/rcpp_test.cpp')
Rcpp::sourceCpp('Desktop/R_entropy/similarity_count.cpp')
Rcpp::sourceCpp('Desktop/R_entropy/similarity_count.cpp')
test = matrix(rnorm(100), ncol = 5)
similarity_count(test, .5)
similarity_count(test, .1)
similarity_count(test, 1)
dist(test, method = "maximum")
similarity_count(test, 1)
x = dist(test, method = "maximum")
sum(x < 1)
install.packages('rbenchmark')
library(rbenchmark)
install.packages('microbenchmark')
library(microbenchmark)
x = matrix(rnorm(100), ncol = 5)
microbenchmark(
similarity_count(test, 1),
sum(dist(test, method = "maximum") == 1)
)
?microbenchmark
microbenchmark(
similarity_count(test, 1),
sum(dist(test, method = "maximum") == 1),
times = 1000
)
y = microbenchmark(
similarity_count(test, 1),
sum(dist(test, method = "maximum") == 1),
times = 1000
)
View(y)
microbenchmark(
similarity_count(test, 1),
sum(dist(test, method = "maximum") == 1),
times = 10000
)
x = matrix(rnorm(1000), ncol = 10)
microbenchmark(
similarity_count(test, 1),
sum(dist(test, method = "maximum") == 1),
times = 1000
)
x = matrix(rnorm(100000), ncol = 100)
microbenchmark(
similarity_count(test, 1),
sum(dist(test, method = "maximum") == 1),
times = 1000
)
microbenchmark(
similarity_count(test, 1),
sum(dist(test, method = "maximum") < 1),
times = 1000
)
sum(dist(test, method = "maximum") < 1)
x = matrix(rnorm(10000), ncol = 100)
x = matrix(rnorm(10000), ncol = 10)
x = matrix(rnorm(10000), ncol = 10)
microbenchmark(
similarity_count(x, 1),
sum(dist(x, method = "maximum") < 1),
times = 1000
)
?C_Cdist
??C_Cdist
x = matrix(rnorm(1000), ncol = 10)
microbenchmark(
similarity_count(x, 1),
sum(dist(x, method = "maximum") < 1),
times = 1000
)
microbenchmark(
similarity_count(x, .5),
sum(dist(x, method = "maximum") < .5),
times = 1000
)
setwd('/Users/areshenk/Documents/Presentation/Teaching-Material/BayesWorkshop/2 - Fitting Models in Stan/code')
write.csv(rnorm(0,1), file = 'example_data.csv')
write.csv(rnorm(100, mean = 0,sd = 1), file = 'example_data.csv')
write.table(rnorm(100, mean = 0,sd = 1), file = 'example_data.csv', sep = ',', row.names = F, col.names = F)
data = read.csv('example_data.csv', header = F)
View(data)
hist(data)
hist(data[,1])
hist(data[,1], fill = 'black')
?hist
hist(data[,1], col = 'black')
stan_data = list(N = dim(data[,1]),
x = data[,1])
?stan
library(rstan)
?stan
stan_fit = stan(file = 'normal_fit.stan',
data = stan_data,
warmup = 50, iter = 100,
chains = 3)
stan_fit = stan(file = 'normal_fit.stan',
data = stan_data,
warmup = 50, iter = 100,
chains = 3)
stan_fit = stan(file = 'normal_fit.stan',
data = stan_data,
warmup = 50, iter = 100,
chains = 3)
stan_data = list(N = dim(data)[1],
x = data[,1])
stan_fit = stan(file = 'normal_fit.stan',
data = stan_data,
warmup = 50, iter = 100,
chains = 3)
library(stan_fit)
print(stan_fit)
traceplot(stan_fit)
arr <- as.array(stan_fit)
names(arr)
stan_fit = stan(file = 'normal_fit.stan',
data = stan_data,
warmup = 10, iter = 20,
chains = 3)
traceplot(stan_fit)
print(stan_fit)
x = seq(from = -1, to = 1, length.out = 100)
y = dnorm(x,
mu = sum(stan_data$x)/(stan_data$N + 1),
sd = sqrt(1/(stan_data$N + 1)))
plot(x,y, xlab = 'Mu', ylab = '',
type = 'l', lty = 2, lwd = 2)
y = dnorm(x,
mean = sum(stan_data$x)/(stan_data$N + 1),
sd = sqrt(1/(stan_data$N + 1)))
plot(x,y, xlab = 'Mu', ylab = '',
type = 'l', lty = 2, lwd = 2)
grid(nx = NULL, ny = NULL)
stan_samples = extract(stan_fit)
hist(stan_samples$mu, add = T, probability = T)
hist(stan_samples$mu, probability = T,
xlab = 'Mu', ylab = '', col = 'black')
hist(stan_samples$mu, probability = T,
xlab = 'Mu', ylab = '', col = 'black',
xlim = c(-1,1))
hist(stan_samples$mu, probability = T,
xlab = 'Mu', ylab = '', col = 'black',
xlim = c(-.5,.5))
grid(nx = NULL, ny = NULL)
x = seq(from = -.5, to = .5, length.out = 100)
y = dnorm(x,
mean = sum(stan_data$x)/(stan_data$N + 1),
sd = sqrt(1/(stan_data$N + 1)))
lines(x,y,
lty = 2, lwd = 2)
hist(stan_samples$mu, probability = T,
xlab = 'Mu', ylab = '', col = 'black',
xlim = c(-.5,.5), ylim = c(0,5))
grid(nx = NULL, ny = NULL)
# Add true posterior
x = seq(from = -.5, to = .5, length.out = 100)
y = dnorm(x,
mean = sum(stan_data$x)/(stan_data$N + 1),
sd = sqrt(1/(stan_data$N + 1)))
lines(x,y,
lty = 2, lwd = 2, col = 'red')
hist(stan_samples$mu, probability = T,
xlab = 'Mu', ylab = '', col = 'black',
xlim = c(-.5,.5), ylim = c(0,5),
main = 'Posterior Samples')
grid(nx = NULL, ny = NULL)
# Add true posterior
x = seq(from = -.5, to = .5, length.out = 100)
y = dnorm(x,
mean = sum(stan_data$x)/(stan_data$N + 1),
sd = sqrt(1/(stan_data$N + 1)))
lines(x,y,
lty = 2, lwd = 2, col = 'red')
legend(x = -.5, y = 5,
lty = 2, col = 'red',
legend = 'True Posterior')
stan_fit = stan(file = 'normal_fit.stan',
data = stan_data,
warmup = 100, iter = 200,
chains = 3)
traceplot(stan_fit)
print(stan_fit)
# Plot samples
stan_samples = extract(stan_fit)
hist(stan_samples$mu, probability = T,
xlab = 'Mu', ylab = '', col = 'black',
xlim = c(-.5,.5), ylim = c(0,5),
main = 'Posterior Samples')
grid(nx = NULL, ny = NULL)
# Add true posterior
x = seq(from = -.5, to = .5, length.out = 100)
y = dnorm(x,
mean = sum(stan_data$x)/(stan_data$N + 1),
sd = sqrt(1/(stan_data$N + 1)))
lines(x,y,
lty = 2, lwd = 2, col = 'red')
legend(x = -.5, y = 5,
lty = 2, col = 'red',
legend = 'True Posterior')
stan_fit = stan(file = 'normal_fit.stan',
data = stan_data,
warmup = 100, iter = 1000,
chains = 3)
traceplot(stan_fit)
print(stan_fit)
# Add true posterior
x = seq(from = -.5, to = .5, length.out = 100)
y = dnorm(x,
mean = sum(stan_data$x)/(stan_data$N + 1),
sd = sqrt(1/(stan_data$N + 1)))
lines(x,y,
lty = 2, lwd = 2, col = 'red')
legend(x = -.5, y = 5,
lty = 2, col = 'red',
legend = 'True Posterior')
# Plot samples
stan_samples = extract(stan_fit)
hist(stan_samples$mu, probability = T,
xlab = 'Mu', ylab = '', col = 'black',
xlim = c(-.5,.5), ylim = c(0,5),
main = 'Posterior Samples')
grid(nx = NULL, ny = NULL)
# Add true posterior
x = seq(from = -.5, to = .5, length.out = 100)
y = dnorm(x,
mean = sum(stan_data$x)/(stan_data$N + 1),
sd = sqrt(1/(stan_data$N + 1)))
lines(x,y,
lty = 2, lwd = 2, col = 'red')
legend(x = -.5, y = 5,
lty = 2, col = 'red',
legend = 'True Posterior')
arr = as.array(stan_fit)
acf(arr[,1,1])
acf(arr[,1,2])
acf(arr[,1,1])
